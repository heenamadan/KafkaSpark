package com.spnotes.spark;

import com.db.mongo.MongoDB;
import com.db.mongo.MongoDBInsertData;
import com.esotericsoftware.kryo.serializers.DefaultSerializers;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.model.Emoji;
import kafka.serializer.StringDecoder;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.function.VoidFunction;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaPairInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;
import scala.Serializable;
import scala.Tuple2;
//import org.apache.spark.streaming.twitter.TwitterUtils;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.io.NotSerializableException;
import java.sql.Connection;
import java.sql.Time;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.Date;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.Properties;
import java.util.Set;


public class KafkaSparkStream implements Serializable{
    public static final long UID = 1L;


    public static void main(String[] args) throws InterruptedException, IOException {
        SparkConf conf = new SparkConf().setMaster("local").setAppName("KafkaSparkStream");
        //JavaSparkContext sc = new JavaSparkContext(sparkConf);
        JavaStreamingContext ssc = new JavaStreamingContext(conf, new       Duration(30));

        Map<String, Object> kafkaParams = new HashMap();
        kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9092");
        kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG,"group2");
        kafkaParams.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"earliest");
        kafkaParams.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,false);
        Collection<String> topics = Arrays.asList("hashtag-twitter-data");



        final JavaInputDStream<ConsumerRecord<String, String>> kafkaSparkPairInputDStream =
                KafkaUtils.createDirectStream(ssc, LocationStrategies.PreferConsistent(), ConsumerStrategies.<String,String>Subscribe(topics,kafkaParams));

        JavaDStream<String> lines = kafkaSparkPairInputDStream.map(new Function<ConsumerRecord<String,String>, String>() {
            public String call(ConsumerRecord<String, String> kafkaRecord) throws Exception {
                return kafkaRecord.value();
            }
        });

        /*JavaDStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
            public Iterator<String> call(String line) throws Exception {
                return Arrays.asList(line.split(" ")).iterator();
            }
        });*/


        Properties props = new Properties();
        props.put("metadata.broker.list", "localhost:9092");
        props.put("bootstrap.servers", "localhost:9092");
        props.put("acks", "all");
        props.put("retries", 0);
        props.put("batch.size", 16384);
        props.put("linger.ms", 1);
        props.put("buffer.memory", 33554432);

        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");





        LinkedHashMap<String,Integer> elementCountMap = new LinkedHashMap();
        lines.foreachRDD(kafkastream->printRDD(elementCountMap, kafkastream));



        // writeToFile(elementCountMap);
        //lines.foreachRDD(KafkaSparkStream::printRDD);


       /* //final Set<String> result = Collections.synchronizedSet(new HashSet<String>());
        StandaloneSender sender = new StandaloneSender();
        wordCounts.foreachRDD((VoidFunction<JavaPairRDD<String, Integer>>) pairRDD -> {
            pairRDD.foreach(new VoidFunction<Tuple2<String,Integer>>() {
                //NotSerializableException nn=new NotSerializableException();
                //Producer<String, String> producer1 = new KafkaProducer<String, String>(props);
                @Override
                public void call(Tuple2<String, Integer> t) throws Exception {
                    System.out.println("value------>"+t._1() + "," + t._2());
                    //sender.send(new ProducerRecord<String, String>("twitter", t._1(), t._2().toString()));
                }

            });
        });*/

        //wordCounts.dstream().saveAsTextFiles("/Users/heena.madan/Downloads/KafkaSpark10-master/hashtagcount1/input", "txt");

        //custom serializer implement
        //https://stackoverflow.com/questions/25914057/task-not-serializable-exception-while-running-apache-spark-job

/*

        Producer<String, String> producer1 = new KafkaProducer<String, String>(kafkaParams);

       wordCounts.foreachRDD(rdd -> {
            rdd.foreachPartition(partitionOfRecords -> {

                System.out.println("partitions->"+rdd.getNumPartitions());
                //System.out.println("partitions->"+rdd.);

                //Producer<String, Integer> producer = MyKafkaProducer.getProducer();
                //Producer<String, String> producer = new KafkaProducer<String, String>(kafkaParams);

                   // producer.send(new ProducerRecord<String, Integer>("test", Integer.toString(j++), partitionOfRecords.next()));
                   //producer.send(new ProducerRecord<String, String>("test", Integer.toString(j++), partitionOfRecords.next()._1));
                    System.out.println("in while loop");
                producer1.send(new ProducerRecord<String, String>("twitter", partitionOfRecords.next()._1, Integer.toString(partitionOfRecords.next()._2)));
                    //System.out.println(partitionOfRecords.next()._2);


            });
        });


*/


        /*hashTags.foreachRDD(rdd->{
            if (!rdd.partitions().isEmpty()) {
                hashTags.dstream().saveAsTextFiles("/Users/heena.madan/Downloads/KafkaSpark10-master/test", "txt");
            }
        });
*/
        //hashTags.dstream().saveAsTextFiles("/Users/heena.madan/Downloads/KafkaSpark10-master/test/input", "txt");
       //atTheRates.dstream().saveAsTextFiles("/Users/heena.madan/Downloads/KafkaSpark10-master/10bje/input", "txt");

        ssc.start();
        ssc.awaitTermination();
    }

    private static void perform(JavaPairDStream<String, Integer> wordCounts){
        Properties props = new Properties();
        props.put("metadata.broker.list", "localhost:9092");
        props.put("bootstrap.servers", "localhost:9092");
        props.put("acks", "all");
        props.put("retries", 0);
        props.put("batch.size", 16384);
        props.put("linger.ms", 1);
        props.put("buffer.memory", 33554432);

        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        System.out.println("in perform");
        try {
            /*wordCounts.foreachRDD(rdd -> {
                rdd.foreachPartition(iter ->{
                    KafkaProducerService kafkaProducer  = new KafkaProducerService("javaworld");
                    while (iter.hasNext()){
                        kafkaProducer.sendMessage(iter.next()._1, iter.next()._2.toString());
                    }
                });

            });*/

            wordCounts.foreachRDD(new VoidFunction<JavaPairRDD<String, Integer>>() {
                @Override
                public void call(JavaPairRDD<String, Integer> arg0) throws Exception {
                    Map<String, Integer> wordCountMap = arg0.collectAsMap();

                    for (String key : wordCountMap.keySet()) {
                        //Here we send event to kafka output topic
                        publishToKafka(key, new Long(wordCountMap.get(key)), "test2",props);
                    }

                }
            });


            System.out.println("added to topic");
        } catch (Exception ex) {
            ex.printStackTrace();

        }
    }


    public static void publishToKafka(String word, Long count, String topic, Properties props) {
        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(props);

        try {
            ObjectMapper mapper = new ObjectMapper();
            //String jsonInString = mapper.writeValueAsString(word + "," + count);
            String key = "\"" + word + "\"";
            String val = "\"" + count + "\"";
            String time = "\"" + new Date() + "\"";
            //String event = "{\"word_stats\":" + jsonInString + "}";
            String event1 = "[{\"key\":" + key + "}, {\"value\":" + val + "}, {\"fetchTime\":" + time + "}]";
           // log.info("Message to send to kafka : {}", event);
            producer.send(new ProducerRecord<String, String>(topic, event1));
            //log.info("Event : " + event + " published successfully to kafka!!");
        } catch (Exception e) {
            System.out.println("Problem while publishing the event to kafka.."+ e.getMessage());
           // log.error("Problem while publishing the event to kafka : " + e.getMessage());
        }
        producer.close();
    }

    public static void publishToKafkaEmojis(String emo, String topic, Properties props) {
        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(props);

        try {
            producer.send(new ProducerRecord<String, String>(topic, emo));
        } catch (Exception e) {
            System.out.println("Problem while publishing the event to kafka.."+ e.getMessage());
            // log.error("Problem while publishing the event to kafka : " + e.getMessage());
        }
        producer.close();
    }

    static MongoDB mongoDBInsertData = new MongoDB();



    public static void printRDD( LinkedHashMap<String,Integer> elementCountMap,final JavaRDD<String> s) throws IOException{
        //s.foreach(System.out::println);

        for(String l: s.collect()) {
            System.out.println("mongo-->"+mongoDBInsertData);
            countEmoji(l, elementCountMap);
        }

        System.out.println("size()->"+elementCountMap.size());
    }


    private static void countEmoji(String str, LinkedHashMap<String,Integer> map) {
        System.out.println("str-->"
        +str);
        List<String> lists=EmojiUtils.extractEmojisAsString(str);
        for(String emo: lists) {
            System.out.println("emoji");
            System.out.println(emo);
            Emoji emModel= new Emoji();
            emModel.setCount(1);
            emModel.setEmoji(emo);

           // publishToKafkaEmojis(emo, )

            mongoDBInsertData.insertOrUpdate(emModel);
        }
    }


    private static void writeToFile(LinkedHashMap<String,Integer> elementCountMap) throws IOException {
        File f1 = new File("/Users/heena.madan/Downloads/KafkaSpark10-master/countemoji.txt");
        if(!f1.exists()) {
            f1.createNewFile();
        }
        FileWriter fileWritter = new FileWriter(f1);
        //FileWriter fileWritter = new FileWriter(f1.getName(),true);
        BufferedWriter bw = new BufferedWriter(fileWritter);
        Iterator<Map.Entry<String,Integer>> intr=elementCountMap.entrySet().iterator();
        while(intr.hasNext()) {
            Map.Entry<String,Integer> entry=intr.next();
         //   bw.write(entry.getKey()+","+entry.getValue());
            System.out.println("emoji counts---"+entry.getKey()+","+entry.getValue());
        }

    }

    private static void writeToFile1(String emo) throws IOException {
        File f1 = new File("/Users/heena.madan/Downloads/KafkaSpark10-master/heena.txt");
        if(!f1.exists()) {
            f1.createNewFile();
        }
        FileWriter fileWritter = new FileWriter(f1);
        //FileWriter fileWritter = new FileWriter(f1.getName(),true);
        BufferedWriter bw = new BufferedWriter(fileWritter);
        bw.write(emo);

    }
}

//https://github.com/stdatalabs/SparkTwitterStreamAnalysis/blob/master/src/main/scala/com/stdatalabs/Streaming/KafkaSparkPopularHashTags.scala